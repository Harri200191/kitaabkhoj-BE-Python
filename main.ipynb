{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.7.72.210:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def is_person_name(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def count_occurrences(soup, target_word):\n",
    "    vicinity_words = [\"book\", \"novel\", \"author\"]\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "    text = content.get_text().lower()\n",
    "\n",
    "    target_word_count = text.count(target_word.lower())\n",
    "\n",
    "    vicinity_counts = Counter()\n",
    "    for vicinity_word in vicinity_words:\n",
    "        vicinity_pattern = re.compile(fr'\\b{vicinity_word.lower()}\\b')\n",
    "        vicinity_counts[vicinity_word] = len(re.findall(vicinity_pattern, text))\n",
    "\n",
    "    return target_word_count, vicinity_counts\n",
    "\n",
    "def process_text_lines(text_lines): \n",
    "    name_lis = []\n",
    "\n",
    "    for line in text_lines:\n",
    "        if is_person_name(line) == True:\n",
    "            name_lis.append(line)\n",
    "\n",
    "    if len(name_lis) > 0:\n",
    "        fin_str = \"\"\n",
    "        for name in name_lis:\n",
    "            fin_str += name + \" \"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return fin_str\n",
    "\n",
    "def search_wikipedia(text_lines): \n",
    "    time.sleep(1)\n",
    "    for line in text_lines:\n",
    "        if is_person_name(line) == True: \n",
    "            continue\n",
    "        else:\n",
    "            search_url = f'https://en.wikipedia.org/wiki/{line}'\n",
    "            response = requests.get(search_url)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "            if soup: \n",
    "                target_word_count, vicinity_counts = count_occurrences(soup, line)\n",
    " \n",
    "                print(f\"\\nOccurrences of the word '{line}' in the article: {target_word_count}\")\n",
    "                print(\"\\nOccurrences in the vicinity:\")\n",
    "                for word, count in vicinity_counts.items():\n",
    "                    print(f\"{word.capitalize()}: {count}\")\n",
    "\n",
    "                if vicinity_counts[\"book\"] + vicinity_counts[\"novel\"] > 30:\n",
    "                    return line\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "                    continue      \n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    return None\n",
    "\n",
    "def is_valid_isbn(isbn): \n",
    "    cleaned_isbn = re.sub(r'[-\\s]', '', isbn) \n",
    "    if re.match(r'^\\d{9}[\\dXx]|\\d{13}$', cleaned_isbn):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_isbn_by_title(title):\n",
    "\n",
    "    load_dotenv() \n",
    "    api_key = os.getenv('GOOGLE_BOOKS_API_KEY')\n",
    "    base_url = 'https://www.googleapis.com/books/v1/volumes'\n",
    " \n",
    "    params = {\n",
    "        'q': f'intitle:{title}',\n",
    "        'key': api_key,\n",
    "    }\n",
    " \n",
    "    response = requests.get(base_url, params=params)\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    " \n",
    "        if 'items' in data: \n",
    "            first_book = data['items'][0]\n",
    "            volume_info = first_book.get('volumeInfo', {})\n",
    "            industry_identifiers = volume_info.get('industryIdentifiers', [])\n",
    "\n",
    "            for identifier in industry_identifiers:\n",
    "                if identifier['type'] == 'ISBN_10' or identifier['type'] == 'ISBN_13':\n",
    "                    return identifier['identifier']\n",
    "                \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_wikipedia_previews(query): \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)  \n",
    " \n",
    "    driver.get('https://en.wikipedia.org/w/index.php?title=Special:Search&profile=default&search=')\n",
    " \n",
    "    search_input = driver.find_element(By.NAME, 'search')\n",
    "    search_input.send_keys(query)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    " \n",
    "    driver.implicitly_wait(5) \n",
    " \n",
    "    page_source = driver.page_source \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    preview_divs = soup.find_all('th', class_=\"infobox-above summary\")\n",
    "\n",
    "    if len(preview_divs) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    " \n",
    "\n",
    "@app.route('/process_text', methods=['GET'])\n",
    "def process_text():\n",
    "    try:\n",
    "        data = request.args.get('txt') \n",
    "        print(data)\n",
    "        data = data.lower()\n",
    "        text_lines = data.split(',')\n",
    "\n",
    "        print(text_lines)\n",
    "        \n",
    "        author = process_text_lines(text_lines)\n",
    "        time.sleep(4)\n",
    "\n",
    "        for line in text_lines:\n",
    "            if (get_wikipedia_previews(line) == True):\n",
    "                book = line\n",
    "                break\n",
    "            else:\n",
    "                book = None\n",
    "                print(\"Nope\") \n",
    "\n",
    "        isbn = get_isbn_by_title(book)\n",
    "        \n",
    "        print(\"Author: \", author)\n",
    "        print(\"Book: \", book)\n",
    "        print(\"ISBN: \", isbn)\n",
    "\n",
    "        return jsonify({'author': author, 'book': book, 'isbn': isbn})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
