{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "def is_person_name(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def count_occurrences(soup, target_word):\n",
    "    vicinity_words = [\"book\", \"novel\", \"author\"]\n",
    "    content = soup.find('div', {'id': 'mw-content-text'})\n",
    "    text = content.get_text().lower()\n",
    "\n",
    "    target_word_count = text.count(target_word.lower())\n",
    "\n",
    "    vicinity_counts = Counter()\n",
    "    for vicinity_word in vicinity_words:\n",
    "        vicinity_pattern = re.compile(fr'\\b{vicinity_word.lower()}\\b')\n",
    "        vicinity_counts[vicinity_word] = len(re.findall(vicinity_pattern, text))\n",
    "\n",
    "    return target_word_count, vicinity_counts\n",
    "\n",
    "def process_text_lines(text_lines): \n",
    "    for line in text_lines:\n",
    "        if is_person_name(line) == True:\n",
    "            return line\n",
    "        \n",
    "    return None\n",
    "\n",
    "def search_wikipedia(text_lines): \n",
    "    time.sleep(1)\n",
    "    for line in text_lines:\n",
    "        if is_person_name(line) == True: \n",
    "            continue\n",
    "        else:\n",
    "            search_url = f'https://en.wikipedia.org/wiki/{line}'\n",
    "            response = requests.get(search_url)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "            if soup: \n",
    "                target_word_count, vicinity_counts = count_occurrences(soup, line)\n",
    " \n",
    "                print(f\"\\nOccurrences of the word '{line}' in the article: {target_word_count}\")\n",
    "                print(\"\\nOccurrences in the vicinity:\")\n",
    "                for word, count in vicinity_counts.items():\n",
    "                    print(f\"{word.capitalize()}: {count}\")\n",
    "\n",
    "                if vicinity_counts[\"book\"] + vicinity_counts[\"novel\"] > 30:\n",
    "                    return line\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "                    continue      \n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    return None\n",
    "\n",
    "def is_valid_isbn(isbn): \n",
    "    cleaned_isbn = re.sub(r'[-\\s]', '', isbn) \n",
    "    if re.match(r'^\\d{9}[\\dXx]|\\d{13}$', cleaned_isbn):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_isbn_by_title(title):\n",
    "\n",
    "    load_dotenv() \n",
    "    api_key = os.getenv('GOOGLE_BOOKS_API_KEY')\n",
    "    base_url = 'https://www.googleapis.com/books/v1/volumes'\n",
    " \n",
    "    params = {\n",
    "        'q': f'intitle:{title}',\n",
    "        'key': api_key,\n",
    "    }\n",
    " \n",
    "    response = requests.get(base_url, params=params)\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    " \n",
    "        if 'items' in data: \n",
    "            first_book = data['items'][0]\n",
    "            volume_info = first_book.get('volumeInfo', {})\n",
    "            industry_identifiers = volume_info.get('industryIdentifiers', [])\n",
    "\n",
    "            for identifier in industry_identifiers:\n",
    "                if identifier['type'] == 'ISBN_10' or identifier['type'] == 'ISBN_13':\n",
    "                    return identifier['identifier']\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_wikipedia_previews(query): \n",
    "    driver = webdriver.Chrome()  \n",
    " \n",
    "    driver.get('https://en.wikipedia.org/w/index.php?title=Special:Search&profile=default&search=')\n",
    " \n",
    "    search_input = driver.find_element(By.NAME, 'search')\n",
    "    search_input.send_keys(query)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    " \n",
    "    driver.implicitly_wait(5) \n",
    " \n",
    "    page_source = driver.page_source \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    preview_divs = soup.find_all('span', id= 'Books_and_stories')\n",
    "\n",
    "    if len(preview_divs) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    " \n",
    "\n",
    "@app.route('/process_text', methods=['GET'])\n",
    "def process_text():\n",
    "    try:\n",
    "        data = request.args.get('txt') \n",
    "        text_lines = data.split('\\n')\n",
    "\n",
    "        author = process_text_lines(text_lines)\n",
    "        for line in text_lines:\n",
    "            if (get_wikipedia_previews(line) == True):\n",
    "                book = line\n",
    "                break\n",
    "            else:\n",
    "                book = None\n",
    "                print(\"Nope\") \n",
    "\n",
    "        isbn = get_isbn_by_title(book)\n",
    "\n",
    "        return jsonify({'author': author, 'book': book, 'isbn': isbn})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
